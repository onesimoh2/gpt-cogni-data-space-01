{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import math\n",
    "import pandas\n",
    "\n",
    "def execute_python(file_path):\n",
    "    # Run the Python file\n",
    "    result = subprocess.run(['python', file_path], capture_output=True, text=True)\n",
    "\n",
    "    # Check if the execution was successful\n",
    "    if result.returncode == 0:\n",
    "        # Get the output from the executed file\n",
    "        output = result.stdout.strip()\n",
    "        #print(\"Output:\", output)\n",
    "    else:\n",
    "        # Print the error message if the execution failed\n",
    "        output = result.stderr.strip()\n",
    "        #print(\"Error:\", error)\n",
    "    return output\n",
    "\n",
    "def get_response_from_gpt4(sys, usr, assist, tmp=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    temperature = tmp,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\", \"content\": usr},\n",
    "        {\"role\": \"assistant\", \"content\": assist}\n",
    "    ]    \n",
    "    \n",
    "    )\n",
    "    #res = (response['choices'][0]['message']['content'])\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_response_from_gpt4_funct(sys, usr, assist, func, tmp=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    #model=\"gpt-3.5-turbo\",\n",
    "    temperature = tmp,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\", \"content\": usr},\n",
    "        {\"role\": \"assistant\", \"content\": assist}\n",
    "    ],\n",
    "    functions = func\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "querystr = \"\"\"Obtain the data and build a fish capture monthly data frame containing years 1986 to 2012\n",
    "The expected columns are: Date which is a datetime, \n",
    "Total (the total dollar captured) which is a number and Wheight representing the wheight of \n",
    "the capture which is a number. The Date column is the primary key for this data set. \n",
    "To solve this query you might have to construct the resulting dataframe by merging, appending and joining \n",
    "sepparate dataframes so the final number of years and the final number of columns might not be obtained \n",
    "in the first steap but rather in multiple ones. Do not create columns or any other data that doesnt exist \n",
    "at the moment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# key01 = \"\"\" Fish capture weekly data containing years 1986 to 2012\n",
    "# The columns are: date in the format MM/dd/yyyy, \n",
    "# Total capture value which is a number and Wheight representing the wheight of \n",
    "# the capture whic is a number. Every row corresponds to a week data.\n",
    "# \"\"\"\n",
    "# available_data_01 = {\n",
    "\n",
    "#     \"name\": \"csv_fish_all_data\",\n",
    "#     \"type\" : \"csv\",\n",
    "#     \"route\": \"data/catfishall.csv\",\n",
    "#     \"query\": \"\",\n",
    "#     \"key\": key01,\n",
    "#     \"value\": -1\n",
    "# }\n",
    "cont = \"\"\"{\n",
    "    'columns': []\n",
    "    }\n",
    "    'constrains': '',\n",
    "    'descriptions': 'This dataframe is currently empty and needs to be created'\n",
    "} \"\"\"\n",
    "requested_data = {\n",
    "    \"dataframe\" : \"Fish_all_data\",\n",
    "    \"query\": querystr,\n",
    "    \"file_path\": \"\",\n",
    "    \"file_path_type\" : \"Pandas data frame\",\n",
    "    \"content\": cont,\n",
    "    \"value\": \"\"\n",
    "}\n",
    "\n",
    "\n",
    "# key01 = \"\"\"Fish capture monthly data containing years 2000 to 2012\n",
    "# It contains only columns Date and Total but only for the selected years \n",
    "# The columns are: Date in the format MM/dd/yyyy and \n",
    "# Total which is a number. \n",
    "# The Date column is the primary key for this data set.\n",
    "# \"\"\"\n",
    "cont1 = \"\"\"{\n",
    "    'columns': {\n",
    "        ['columnname': 'Date',\n",
    "        'format': datetime,\n",
    "        'primarykey': 'yes'\n",
    "        ],\n",
    "        [\n",
    "        'columnname': 'Total',\n",
    "        'format': 'number',\n",
    "        'primarykey': 'no'\n",
    "        ]\n",
    "    }\n",
    "    'constrains': 'Years from 2000 to 2012',\n",
    "    'descriptions': 'Contains only years 2000 to 2012 and only columns Date and Total' \n",
    "} \"\"\"\n",
    "available_data_01 = {\n",
    "\n",
    "    \"dataframe\": \"data_from_2000_on\",\n",
    "    \"file_path\": \"data/catfich-from_2000.csv\",\n",
    "    \"file_path_type\" : \"csv\",   \n",
    "    \"query\": \"\",\n",
    "    \"content\": cont1,\n",
    "    \"value\": -1\n",
    "}\n",
    "\n",
    "# key02 = \"\"\" Fish capture monthly data containing years 1986 to 1999. \n",
    "# It contains only columns Date and Total but only for the selected years \n",
    "# The columns are: Date in the format MM/dd/yyyy and  \n",
    "# Total which is a number. \n",
    "# The Date column is the primary key for this data set.\n",
    "# \"\"\"\n",
    "cont2 = \"\"\"{\n",
    "    'columns': {\n",
    "        ['columnname': 'Date',\n",
    "        'format': datetime,\n",
    "        'primarykey': 'yes'\n",
    "        ],\n",
    "        [\n",
    "        'columnname': 'Total',\n",
    "        'format': 'number',\n",
    "        'primarykey': 'no'\n",
    "        ]\n",
    "    }\n",
    "    'constrains': 'Years from 1986 to 1999',\n",
    "    'descriptions': 'Contains only years 1986 to 1999 and only columns Date and Total' \n",
    "} \"\"\"\n",
    "\n",
    "available_data_02 = {\n",
    "    \"dataframe\" : \"fish_from_1986_up_tp_1999\",\n",
    "    \"file_path\": \"data/catfich-until1999.csv\",\n",
    "    \"file_path_type\" : \"csv\",   \n",
    "    \"query\": \"\",\n",
    "    \"content\": cont2,\n",
    "    \"value\": -1\n",
    "}\n",
    "\n",
    "# key03 = \"\"\" Fish capture monthly data containing all years years 1986 to 2012 \n",
    "# but it only contains column date and Wheight\n",
    "# The columns are: Date in the format MM/dd/yyyy and\n",
    "# Wheight which is a number. \n",
    "# The Date column is the primary key for this data set.\n",
    "#\"\"\"\n",
    "cont3 = \"\"\"{\n",
    "    'columns': {\n",
    "        ['columnname': 'Date',\n",
    "        'format': datetime,\n",
    "        'primarykey': 'yes'\n",
    "        ],\n",
    "        [\n",
    "        'columnname': 'Wheight',\n",
    "        'format': 'number',\n",
    "        'primarykey': 'no'\n",
    "        ]\n",
    "    }\n",
    "    'constrains': 'Years from 1986 to 2012',\n",
    "    'descriptions': 'Contains all the years but only columns Date and Wheight'\n",
    "} \"\"\"\n",
    "\n",
    "available_data_03 = {\n",
    "    \"dataframe\" : \"fish_containing_wheight\",\n",
    "    \"file_path\": \"data/catfish-Allyears-onlywheight.csv\",\n",
    "    \"file_path_type\" : \"csv\",   \n",
    "    \"query\": \"\",\n",
    "    \"content\": cont3,\n",
    "    \"value\": -1\n",
    "}\n",
    "\n",
    "metadata_lst = [    \n",
    "    json.dumps(requested_data),\n",
    "    json.dumps(available_data_01),\n",
    "    json.dumps(available_data_02),\n",
    "    json.dumps(available_data_03)\n",
    "]\n",
    "\n",
    "found_metadata_list = []\n",
    "\n",
    "load_dotenv(\"C:\\Projrcts\\Python\\MSJupiter\\myenv.env\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys = \"\"\" You are a good data analyst assistant and have a good understanding \n",
    "about tables, dataframes, csv and data structure in general\n",
    "\"\"\"\n",
    "my_string = \" \".join(metadata_lst)\n",
    "usr = \" You have to consider the elements in the following list \" + my_string + \"\"\" \n",
    " The first element of the list is the dataframe that we want to construct, the others  \n",
    " data sets are the ones that could be used to build the the first one. You have to consider the content of\n",
    " each  data set to determine if it is relevant or not to build the dataframe specified in the first\n",
    "  element of the list\"\"\"\n",
    "\n",
    "assist = \" Consider the following query: \" + querystr + \"\"\" Use this query to find the provided\n",
    " list which is the most relevant to build the dataframe specified in the query. For that compare eqch element \n",
    " of the list comparing the content attribute with the provided query and the content value of the first element of the list.\n",
    " Calculate a value between 0 and 5 where 5 is the most valued one, for this compare the content attribute \n",
    " of the first dataframe with the rest and assign more value to the one providing more new data. Provide me the list of the dataframe \n",
    " and value attributes of each element. Remember the first element is the one we want to build so the values have to be\n",
    " calculated for the second element on. Please provide the result in a Json format. The Json should look like this, do not add any additional explanation:\n",
    " [\n",
    "  {\"dataframe\": 'The dataframe name',\"file_path\":'the file_path',\"file_path_type\":'file_path_type', \"content\": \"the content attribute\" \"value\": ?}, {...}\n",
    " ]\n",
    " Do not provide any aditional explanation, Only the Json object that I requested.\n",
    "\"\"\"\n",
    "#print(usr)\n",
    "response = get_response_from_gpt4(sys, usr, assist)\n",
    "response_message = response[\"choices\"][0][\"message\"]\n",
    "cont = response_message[\"content\"]\n",
    "result_table = json.loads(str(cont))\n",
    "ref1 = json.dumps(requested_data)\n",
    "\n",
    "max_element_df_name = max(result_table, key=lambda x: x['value'] if x['dataframe'] not in found_metadata_list else float('-inf'))\n",
    "\n",
    "#max_element = json.dumps(max(result_table, key=lambda x: x['value']))\n",
    "max_element = json.dumps(max_element_df_name)\n",
    "max_df = max_element_df_name[\"dataframe\"]\n",
    "found_metadata_list.append(max_df)\n",
    "#print(cont)\n",
    "\n",
    "# Construct the Python function \n",
    "sys1 = \"\"\"You are an accomplished data analyst assistant with the ability to write Python scripts for data \n",
    "manipulation tasks. You are expected to use various sources of data to gradually build the requested \n",
    "final dataframe described in the query. It's important to note that the result does not need to be \n",
    "achieved in a single step. Instead, you should focus on incrementally constructing the resulting \n",
    "dataframe based on the available data. Avoid creating elements or adding columns/rows that are not \n",
    "provided in the data or NaN. The goal is to build the dataframe in accordance with the query, ensuring that \n",
    "the content aligns with the available data sources. Overall, you are expected to leverage your skills \n",
    "and expertise in data analysis and utilize pandas to read CSV files, perform various dataframe \n",
    "operations, and incrementally construct the desired dataframe based on the query specifications and \n",
    "the available data. \n",
    "\"\"\"\n",
    "assist1 = \"\"\" You will receive a query and two metadata references describing two different dataframes,the \n",
    "first is the resulting dataframe and the second is the one providing additional data to be included \n",
    "into the first one. The query contains the expected final result, and the references has the following \n",
    "attributes: dataframe: the name of the dataframe to be created, query: the description of the expected \n",
    "final result, file_path: the name of the file to be read into a dataframe, the file_path_type: \n",
    "typically csv, content: describing the content of that dataframe more specifically the columns of the dataframe and value: you can ignore this attribute. \n",
    "The first reference 'reference1' is the description of the resulting dataframe that you need to build, \n",
    "the second 'reference2' is the reference to the dataframe that will provide new data to augment the \n",
    "dataframe described in 'reference1'.The task at hand involves the creation of a Python script that \n",
    "incorporates the data from the 'reference2' dataframe into the resulting dataframe described in \n",
    "'reference1'. This requires implementing dataframe operations to combine the two dataframes, \n",
    "ensuring that the resulting dataframe reflects the changes made after incorporating the data from \n",
    "'reference2'.To accomplish this, the Python script needs to perform the necessary dataframe operations, \n",
    "like join, concat and others, never use'append' nor merge. Use join every time you need to create or update a column based on a common key column \n",
    "considering the content attribute of 'reference2' and the expected final result specified in the query \n",
    "attribute of 'reference1', very important is to know which columns contain each dataset. These operations should be designed to integrate the data from 'reference2' \n",
    "into the resulting dataframe and generate a new content that accurately reflects the changes. It is \n",
    "crucial to pay attention to the provided instructions and the content of each dataframe and ensure that the resulting dataframe adheres \n",
    "to the required structure and contains the expected columns and data for every step\n",
    "The Python script should be crafted meticulously, incorporating the necessary dataframe operations to \n",
    "combine the dataframes, update the resulting dataframe, and generate the desired final output.In summary, \n",
    "the creation of the Python script must involve thoughtful dataframe operations to incorporate the data \n",
    "from 'reference2' into the resulting dataframe described in 'reference1'. The script should generate a \n",
    "new content for 'reference1' that accurately reflects the changes made after integrating the data from \n",
    "'reference2' This new content must be in the json format  like the one in referenc1 and reference2. \n",
    "Do not include any Python comments in the Python script you create.\n",
    "\"\"\"\n",
    "\n",
    "usr1 = \"Use the query and the two references to generate a Python script to oftain a resultinng dataset and the  new content for that dataset; query: \" + querystr + \", reference1: \" + ref1 + \", reference2: \" + max_element \n",
    "usr1 = usr1 + \"\"\". You are going to do an operation of the type df1 = P(df1, df2) where df1 is the dataframe described in ‘reference1’, df2 is the dataframe described in ‘reference2’ and P is the Python script you need to generate to augment df1 with the content of df2. The type of dataframe operations in P will depend on the ‘content’ attribute in ‘reference1’ and reference2’ and the query, following there are a group of operations you want to consider: create db1 when 'file_path’ is empty. Read db1 and db2 from 'file_path’ if it is not empty. Perform an outer join if there are no common columns between db1 and db2 other than the primary key in this case you don’t need to use lsuffix, rsuffix and combine_first. Perform inner, left, or right join when there are common columns and then do combine_first to reduce columns generated with lsuffix and rsuffix to the proper column name as described in the query. Only use valid Pandas dataframe operations, append is not a valid one. At the end of the script, you need to save df1 into a csv file with name equals to the ‘dataframe’ attribute of ‘reference1’. Do not include comments, using '#' in the generated Python code, only Python commands. \n",
    "\"\"\"\n",
    "\n",
    "func = [\n",
    "            {\n",
    "                \"name\": \"execute_python\",\n",
    "                \"description\": \"Execute the Python code generated by GPT\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"pythonscript\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The Python script containing the computation and workflow generated by GPT when no creation or join is necessary. The last part of the script is a renaiming of the columns to make it compatible with the request.\",\n",
    "                        },\n",
    "                        \"newcontent\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The new content for the resulting dataframe. Is the content of reference1 augmented withe the content of reference2. \",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"pythonscript\", \"newcontent\"],\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"execute_operation_between_rederence1_and_reference2\",\n",
    "                \"description\": \"execute the Python code that reads reference1 dataframe and augmente it with the dataframe in reference2\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"pythonscript\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The Python script containing the computation and workflow generated by GPT. Start by reading the csv file containing df1 and df2.\",\n",
    "                        },\n",
    "                        \"newcontent\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The new content for the resulting dataframe. Is the content of reference1 augmented withe the content of reference2. \",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"execute_dataframe_left_join\", \"newcontent\"],\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"execute_create_generate_first_time_from_reference2\",\n",
    "                \"description\": \"Execute the Python code to generate, create and populate, the result dataframe, only if 'file_path' is empty, directly copying the one described in reference2  and not generating columns not  or renaming columns artificially\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"pythonscript\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The Python script containing the computation and workflow generated by GPT. The last part of the script is a renaiming of the columns to make it compatible with the request.\",\n",
    "                        },\n",
    "                        \"newcontent\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The new content for the resulting dataframe. Is the content of reference1 augmented withe the content of reference2. \",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"execute_dataframe_create_result_first_time\", \"newcontent\"],\n",
    "                }\n",
    "            }            \n",
    "        ]  \n",
    "response1 = get_response_from_gpt4_funct(sys1, usr1, assist1, func)\n",
    "response_message = response1[\"choices\"][0][\"message\"]\n",
    "\n",
    "if response_message.get(\"function_call\"):\n",
    "    function_name = response_message[\"function_call\"][\"name\"]\n",
    "    print(\"function_name:\", function_name)\n",
    "    #text = response_message[\"function_call\"][\"arguments\"]\n",
    "    #print(text)\n",
    "    function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "    # Write the file with the generated script\n",
    "    text = function_args['pythonscript']\n",
    "    newcont = function_args['newcontent']\n",
    "    lines = text.split('\\n')\n",
    "    # Write the lines to the file\n",
    "    with open('data/add_numbers.py', 'w') as file:\n",
    "        for line in lines:\n",
    "            file.write(line + '\\n')\n",
    "        \n",
    "    \n",
    "    requested_data[\"file_path\"] = \"Fish_all_data.csv\"\n",
    "    requested_data[\"file_path_type\"] = \"csv\"\n",
    "    requested_data[\"content\"] = newcont\n",
    "    metadata_lst = [    \n",
    "        json.dumps(requested_data),\n",
    "        json.dumps(available_data_01),\n",
    "        json.dumps(available_data_02),\n",
    "        json.dumps(available_data_03)\n",
    "]\n",
    "    print(function_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m# Save the resulting dataframe to a csv file\u001b[39;00m\n\u001b[0;32m     21\u001b[0m df1\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mFish_all_data.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[39mprint\u001b[39;49m(res)\n",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m# Save the resulting dataframe to a csv file\u001b[39;00m\n\u001b[0;32m     21\u001b[0m df1\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mFish_all_data.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[39mprint\u001b[39;49m(res)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Projrcts\\Python\\MSJupiter\\gpt-cogni-data-space-01\\venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projrcts\\Python\\MSJupiter\\gpt-cogni-data-space-01\\venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataframes from the csv files\n",
    "df1 = pd.read_csv('Fish_all_data.csv')\n",
    "df2 = pd.read_csv('data/catfich-until1999.csv')\n",
    "\n",
    "# Ensure that 'Date' is of datetime type in both dataframes\n",
    "df1['Date'] = pd.to_datetime(df1['Date'])\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "\n",
    "# Perform an outer join on the 'Date' column\n",
    "df1 = df1.set_index('Date').join(df2.set_index('Date'), how='outer', lsuffix='_df1', rsuffix='_df2')\n",
    "\n",
    "# Combine the 'Total' columns from both dataframes\n",
    "df1['Total'] = df1['Total_df1'].combine_first(df1['Total_df2'])\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "df1 = df1.drop(columns=['Total_df1', 'Total_df2'])\n",
    "\n",
    "# Save the resulting dataframe to a csv file\n",
    "df1.to_csv('Fish_all_data.csv')\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
