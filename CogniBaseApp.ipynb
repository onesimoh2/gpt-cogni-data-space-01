{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import math\n",
    "import pandas\n",
    "\n",
    "def execute_python(file_path):\n",
    "    # Run the Python file\n",
    "    result = subprocess.run(['python', file_path], capture_output=True, text=True)\n",
    "\n",
    "    # Check if the execution was successful\n",
    "    if result.returncode == 0:\n",
    "        # Get the output from the executed file\n",
    "        output = result.stdout.strip()\n",
    "        #print(\"Output:\", output)\n",
    "    else:\n",
    "        # Print the error message if the execution failed\n",
    "        output = result.stderr.strip()\n",
    "        #print(\"Error:\", error)\n",
    "    return output\n",
    "\n",
    "def get_response_from_gpt4(sys, usr, assist, tmp=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    temperature = tmp,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\", \"content\": usr},\n",
    "        {\"role\": \"assistant\", \"content\": assist}\n",
    "    ]    \n",
    "    \n",
    "    )\n",
    "    #res = (response['choices'][0]['message']['content'])\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_response_from_gpt4_funct(sys, usr, assist, func, tmp=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    #model=\"gpt-3.5-turbo\",\n",
    "    temperature = tmp,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\", \"content\": usr},\n",
    "        {\"role\": \"assistant\", \"content\": assist}\n",
    "    ],\n",
    "    functions = func\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "querystr = \"\"\"Obtain the data and build a fish capture monthly data frame containing years 1986 to 2012\n",
    "The expected columns are: Date in the format MM/dd/yyyy, \n",
    "Total (the total dollar captured) which is a number and Wheight representing the wheight of \n",
    "the capture which is a number. The Date column is the primary key for this data set. \n",
    "To solve this query you might have to construct the resulting dataframe by merging, appending and joining \n",
    "sepparate dataframes so the final number of years and the final number of columns might not be obtained \n",
    "in the first steap but rather in multiple ones. Do not create columns or any other data that doesnt exist \n",
    "at the moment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# key01 = \"\"\" Fish capture weekly data containing years 1986 to 2012\n",
    "# The columns are: date in the format MM/dd/yyyy, \n",
    "# Total capture value which is a number and Wheight representing the wheight of \n",
    "# the capture whic is a number. Every row corresponds to a week data.\n",
    "# \"\"\"\n",
    "# available_data_01 = {\n",
    "\n",
    "#     \"name\": \"csv_fish_all_data\",\n",
    "#     \"type\" : \"csv\",\n",
    "#     \"route\": \"data/catfishall.csv\",\n",
    "#     \"query\": \"\",\n",
    "#     \"key\": key01,\n",
    "#     \"value\": -1\n",
    "# }\n",
    "cont = \"\"\"{\n",
    "    'columns': []\n",
    "    }\n",
    "    'constrains': '',\n",
    "    'descriptions': 'This dataframe is currently empty and needs to be created'\n",
    "} \"\"\"\n",
    "requested_data = {\n",
    "    \"dataframe\" : \"Fish_all_data\",\n",
    "    \"query\": querystr,\n",
    "    \"file_path\": \"\",\n",
    "    \"file_path_type\" : \"Pandas data frame\",\n",
    "    \"content\": cont,\n",
    "    \"value\": \"\"\n",
    "}\n",
    "\n",
    "\n",
    "# key01 = \"\"\"Fish capture monthly data containing years 2000 to 2012\n",
    "# It contains only columns Date and Total but only for the selected years \n",
    "# The columns are: Date in the format MM/dd/yyyy and \n",
    "# Total which is a number. \n",
    "# The Date column is the primary key for this data set.\n",
    "# \"\"\"\n",
    "cont1 = \"\"\"{\n",
    "    'columns': {\n",
    "        ['columnname': 'Date',\n",
    "        'format': 'MM/dd/yyyy',\n",
    "        'primarykey': 'yes'\n",
    "        ],\n",
    "        [\n",
    "        'columnname': 'Total',\n",
    "        'format': 'number',\n",
    "        'primarykey': 'no'\n",
    "        ]\n",
    "    }\n",
    "    'constrains': 'Years from 2000 to 2012',\n",
    "    'descriptions': 'Contains only years 2000 to 2012 and only columns Date and Total' \n",
    "} \"\"\"\n",
    "available_data_01 = {\n",
    "\n",
    "    \"dataframe\": \"data_from_2000_on\",\n",
    "    \"file_path\": \"data/catfich-from_2000.csv\",\n",
    "    \"file_path_type\" : \"csv\",   \n",
    "    \"query\": \"\",\n",
    "    \"content\": cont1,\n",
    "    \"value\": -1\n",
    "}\n",
    "\n",
    "# key02 = \"\"\" Fish capture monthly data containing years 1986 to 1999. \n",
    "# It contains only columns Date and Total but only for the selected years \n",
    "# The columns are: Date in the format MM/dd/yyyy and  \n",
    "# Total which is a number. \n",
    "# The Date column is the primary key for this data set.\n",
    "# \"\"\"\n",
    "cont2 = \"\"\"{\n",
    "    'columns': {\n",
    "        ['columnname': 'Date',\n",
    "        'format': 'MM/dd/yyyy',\n",
    "        'primarykey': 'yes'\n",
    "        ],\n",
    "        [\n",
    "        'columnname': 'Total',\n",
    "        'format': 'number',\n",
    "        'primarykey': 'no'\n",
    "        ]\n",
    "    }\n",
    "    'constrains': 'Years from 1986 to 1999',\n",
    "    'descriptions': 'Contains only years 1986 to 1999 and only columns Date and Total' \n",
    "} \"\"\"\n",
    "\n",
    "available_data_02 = {\n",
    "    \"dataframe\" : \"fish_from_1986_up_tp_1999\",\n",
    "    \"file_path\": \"data/catfich-until1999.csv\",\n",
    "    \"file_path_type\" : \"csv\",   \n",
    "    \"query\": \"\",\n",
    "    \"content\": cont2,\n",
    "    \"value\": -1\n",
    "}\n",
    "\n",
    "# key03 = \"\"\" Fish capture monthly data containing all years years 1986 to 2012 \n",
    "# but it only contains column date and Wheight\n",
    "# The columns are: Date in the format MM/dd/yyyy and\n",
    "# Wheight which is a number. \n",
    "# The Date column is the primary key for this data set.\n",
    "#\"\"\"\n",
    "cont3 = \"\"\"{\n",
    "    'columns': {\n",
    "        ['columnname': 'Date',\n",
    "        'format': 'MM/dd/yyyy',\n",
    "        'primarykey': 'yes'\n",
    "        ],\n",
    "        [\n",
    "        'columnname': 'Wheight',\n",
    "        'format': 'number',\n",
    "        'primarykey': 'no'\n",
    "        ]\n",
    "    }\n",
    "    'constrains': 'Years from 1986 to 2012',\n",
    "    'descriptions': 'Contains all the years but only columns Date and Wheight'\n",
    "} \"\"\"\n",
    "\n",
    "available_data_03 = {\n",
    "    \"dataframe\" : \"fish_containing_wheight\",\n",
    "    \"file_path\": \"data/catfish-Allyears-onlywheight.csv\",\n",
    "    \"file_path_type\" : \"csv\",   \n",
    "    \"query\": \"\",\n",
    "    \"content\": cont3,\n",
    "    \"value\": -1\n",
    "}\n",
    "\n",
    "metadata_lst = [    \n",
    "    json.dumps(requested_data),\n",
    "    json.dumps(available_data_01),\n",
    "    json.dumps(available_data_02),\n",
    "    json.dumps(available_data_03)\n",
    "]\n",
    "\n",
    "found_metadata_list = []\n",
    "\n",
    "load_dotenv(\"C:\\Projrcts\\Python\\MSJupiter\\myenv.env\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys = \"\"\" You are a good data analyst assistant and have a good understanding \n",
    "about tables, dataframes, csv and data structure in general\n",
    "\"\"\"\n",
    "my_string = \" \".join(metadata_lst)\n",
    "usr = \" You have to consider the elements in the following list \" + my_string + \"\"\" \n",
    " The first element of the list is the dataframe that we want to construct, the others  \n",
    " data sets are the ones that could be used to build the the first one. You have to consider the content of\n",
    " each  data set to determine if it is relevant or not to build the dataframe specified in the first\n",
    "  element of the list\"\"\"\n",
    "\n",
    "assist = \" Consider the following query: \" + querystr + \"\"\" Use this query to find the provided\n",
    " list which is the most relevant to build the dataframe specified in the query. For that compare eqch element \n",
    " of the list comparing the content attribute with the provided query and the content value of the first element of the list.\n",
    " Calculate a value between 0 and 5 where 5 is the most valued one, for this compare the content attribute \n",
    " of the first dataframe with the rest and assign more value to the one providing more new data. Provide me the list of the dataframe \n",
    " and value attributes of each element. Remember the first element is the one we want to build so the values have to be\n",
    " calculated for the second element on. Please provide the result in a Json format. The Json should look like this, do not add any additional explanation:\n",
    " [\n",
    "  {\"dataframe\": 'The dataframe name',\"file_path\":'the file_path',\"file_path_type\":'file_path_type', \"content\": \"the content attribute\" \"value\": ?}, {...}\n",
    " ]\n",
    " Do not provide any aditional explanation, Only the Json object that I requested.\n",
    "\"\"\"\n",
    "#print(usr)\n",
    "response = get_response_from_gpt4(sys, usr, assist)\n",
    "response_message = response[\"choices\"][0][\"message\"]\n",
    "cont = response_message[\"content\"]\n",
    "result_table = json.loads(str(cont))\n",
    "ref1 = json.dumps(requested_data)\n",
    "\n",
    "max_element_df_name = max(result_table, key=lambda x: x['value'] if x['dataframe'] not in found_metadata_list else float('-inf'))\n",
    "\n",
    "#max_element = json.dumps(max(result_table, key=lambda x: x['value']))\n",
    "max_element = json.dumps(max_element_df_name)\n",
    "max_df = max_element_df_name[\"dataframe\"]\n",
    "found_metadata_list.append(max_df)\n",
    "#print(cont)\n",
    "\n",
    "# Construct the Python function \n",
    "sys1 = \"\"\"You are an accomplished data analyst assistant with the ability to write Python scripts for data \n",
    "manipulation tasks. You are expected to use various sources of data to gradually build the requested \n",
    "final dataframe described in the query. It's important to note that the result does not need to be \n",
    "achieved in a single step. Instead, you should focus on incrementally constructing the resulting \n",
    "dataframe based on the available data. Avoid creating elements or adding columns/rows that are not \n",
    "provided in the data. The goal is to build the dataframe in accordance with the query, ensuring that \n",
    "the content aligns with the available data sources. Overall, you are expected to leverage your skills \n",
    "and expertise in data analysis and utilize pandas to read CSV files, perform various dataframe \n",
    "operations, and incrementally construct the desired dataframe based on the query specifications and \n",
    "the available data. \n",
    "\"\"\"\n",
    "usr1 = \"\"\" You will receive a query and two metadata references describing two different dataframes,the \n",
    "first is the resulting dataframe and the second is the one providing additional data to be included \n",
    "into the first one. The query contains the expected final result, and the references has the following \n",
    "attributes: dataframe: the name of the dataframe to be created, query: the description of the expected \n",
    "final result, file_path: the name of the file to be read into a dataframe, the file_path_type: \n",
    "typically csv, content: describing the content of that dataframe, value: you can ignore this attribute. \n",
    "The first reference 'reference1' is the description of the resulting dataframe that you need to build, \n",
    "the second 'reference2' is the reference to the dataframe that will provide new data to build the \n",
    "dataframe described in 'reference1'.The task at hand involves the creation of a Python script that \n",
    "incorporates the data from the 'reference2' dataframe into the resulting dataframe described in \n",
    "'reference1'. This requires implementing dataframe operations to combine the two dataframes, \n",
    "ensuring that the resulting dataframe reflects the changes made after incorporating the data from \n",
    "'reference2'.To accomplish this, the Python script needs to perform the necessary dataframe operations, \n",
    "like join, concat and others, never use'append' nor merge. Use join evry time you need to create or update a column based on a key \n",
    "considering the content attribute of 'reference2' and the expected final result specified in the query \n",
    "attribute of 'reference1'. These operations should be designed to integrate the data from 'reference2' \n",
    "into the resulting dataframe and generate a new content that accurately reflects the changes. It is \n",
    "crucial to pay attention to the provided instructions and ensure that the resulting dataframe adheres \n",
    "to the required structure and contains the expected columns and data for every step\n",
    "The Python script should be crafted meticulously, incorporating the necessary dataframe operations to \n",
    "combine the dataframes, update the resulting dataframe, and generate the desired final output.In summary, \n",
    "the creation of the Python script must involve thoughtful dataframe operations to incorporate the data \n",
    "from 'reference2' into the resulting dataframe described in 'reference1'. The script should generate a \n",
    "new content for 'reference1' that accurately reflects the changes made after integrating the data from \n",
    "'reference2' This new content must be reaten in the json format  like the one in referenc1 and reference2. \n",
    "Do not include any Python comments in the Python script you create.\n",
    "\"\"\"\n",
    "\n",
    "assist1 = \"Use the query and the two references to oftain a resultinng dataset and the  new content for that dataset query: \" + querystr + \", reference1: \" + ref1 + \", reference2: \" + max_element \n",
    "assist1 = assist1 + \"\"\". Analyze very carefully the 'content' attribute to understand the data contained in every dataframe to plan the required steps and commands to be included in the Python program. The typical commands to be used are join, concat, assignments and copy, use join when you need to add or modify columns based on a primary key, never use append or merge. It is especially important to rearrange the columns of the resulting file according to the expected updated content for example after a merge similar columns could be created you need to reduce the columns to the expected ones. At the end of the script save the dataframe in 'reference1' into a csv file, the name of the csv is the name of the file plus .csv. If the dataframe in 'reference1' has being saved, it exists as a file, then you must start the Python program reading its '.csv' file. Do not include comments, using '#' in the generated Python code, only Python commands. \n",
    "\"\"\"\n",
    "\n",
    "func = [\n",
    "            {\n",
    "                \"name\": \"execute_python\",\n",
    "                \"description\": \"Execute the Python code generated by GPT\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"pythonscript\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The Python script containing the computation and workflow generated by GPT. \",\n",
    "                        },\n",
    "                        \"newcontent\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The new content for the resulting dataframe. Is the content of reference1 augmented withe the content of reference2. \",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"pythonscript\", \"newcontent\"],\n",
    "                },\n",
    "            }\n",
    "        ]  \n",
    "response1 = get_response_from_gpt4_funct(sys1, usr1, assist1, func)\n",
    "response_message = response1[\"choices\"][0][\"message\"]\n",
    "\n",
    "if response_message.get(\"function_call\"):\n",
    "    function_name = response_message[\"function_call\"][\"name\"]\n",
    "    print(\"function_name:\", function_name)\n",
    "    #text = response_message[\"function_call\"][\"arguments\"]\n",
    "    #print(text)\n",
    "    function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "    # Write the file with the generated script\n",
    "    text = function_args['pythonscript']\n",
    "    newcont = function_args['newcontent']\n",
    "    lines = text.split('\\n')\n",
    "    # Write the lines to the file\n",
    "    with open('data/add_numbers.py', 'w') as file:\n",
    "        for line in lines:\n",
    "            file.write(line + '\\n')\n",
    "        file.write('print(result)' + '\\n')\n",
    "    \n",
    "    requested_data[\"content\"] = newcont\n",
    "    metadata_lst = [    \n",
    "        json.dumps(requested_data),\n",
    "        json.dumps(available_data_01),\n",
    "        json.dumps(available_data_02),\n",
    "        json.dumps(available_data_03)\n",
    "]\n",
    "    print(function_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m Fish_all_data \u001b[39m=\u001b[39m Fish_all_data\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mjoin(fish_from_1986_up_tp_1999\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[39m# Reset the index\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m Fish_all_data\u001b[39m.\u001b[39mreset_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[39m# Save the resulting dataframe to a csv file\u001b[39;00m\n\u001b[0;32m     20\u001b[0m Fish_all_data\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mFish_all_data.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[52], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m Fish_all_data \u001b[39m=\u001b[39m Fish_all_data\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mjoin(fish_from_1986_up_tp_1999\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[39m# Reset the index\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m Fish_all_data\u001b[39m.\u001b[39mreset_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[39m# Save the resulting dataframe to a csv file\u001b[39;00m\n\u001b[0;32m     20\u001b[0m Fish_all_data\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mFish_all_data.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Projrcts\\Python\\MSJupiter\\gpt-cogni-data-space-01\\venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projrcts\\Python\\MSJupiter\\gpt-cogni-data-space-01\\venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data from the csv file\n",
    "fish_from_1986_up_tp_1999 = pd.read_csv('data/catfich-until1999.csv')\n",
    "\n",
    "# If the Fish_all_data dataframe exists, read it from the csv file\n",
    "try:\n",
    "    Fish_all_data = pd.read_csv('Fish_all_data.csv')\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create an empty dataframe with the required columns\n",
    "    Fish_all_data = pd.DataFrame(columns=['Date', 'Wheight'])\n",
    "\n",
    "# Join the two dataframes on the 'Date' column\n",
    "Fish_all_data = Fish_all_data.set_index('Date').join(fish_from_1986_up_tp_1999.set_index('Date'))\n",
    "\n",
    "# Reset the index\n",
    "Fish_all_data.reset_index(inplace=True)\n",
    "\n",
    "# Save the resulting dataframe to a csv file\n",
    "Fish_all_data.to_csv('Fish_all_data.csv', index=False)\n",
    "print(\"OK\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
